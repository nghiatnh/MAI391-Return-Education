{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    f = open(fname, \"rb\")\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading object file is faster than loading xlsx file\n",
    "data = load_data(\"wagedata.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to train model\n",
    "data_vars = [\"educ\",\"exper\",\"expersq\",\"KWW\",\"black\",\"smsa\",\"enroll\"]\n",
    "# Remove NaN rows \n",
    "data_ = data[data_vars + [\"lwage\"]].dropna()\n",
    "# Just use some variable in data\n",
    "X_data = data_[data_vars]\n",
    "# Add ones to first column of matrix\n",
    "# X = np.concatenate((np.ones(shape=(data_.shape[0],1)) , np.array(X_data)), axis=1)\n",
    "X = X_data.T\n",
    "# Log wage is the output\n",
    "Y = np.array(data_[\"lwage\"]).reshape(len(data_[\"lwage\"]), 1).T\n",
    "# Split data to train and test, shuffle the data\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# print(\"X shape: {0} \\nY shape: {1}\".format(X_train.shape, Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost or loss function  \n",
    "def cost(Y, Yhat):    \n",
    "    N = X.shape[1]\n",
    "    return 0.5 * np.linalg.norm(Y - Yhat) ** 2 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 19.473492\n",
      "iter 1000, loss: 0.097386\n",
      "iter 2000, loss: 0.097386\n"
     ]
    }
   ],
   "source": [
    "d0 = X.shape[0]\n",
    "d1 = h = 30 # size of hidden layer \n",
    "d2 = C = 1\n",
    "# initialize parameters randomely \n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "# X = X.T # each column of X is a data point \n",
    "# Y = convert_labels(y, C)\n",
    "N = X.shape[1]\n",
    "eta = 0.1 # learning rate \n",
    "lastloss = -1\n",
    "for i in range(10000):\n",
    "    ## Feedforward \n",
    "    Z1 = np.dot(W1.T, X) + b1 \n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    # import pdb; pdb.set_trace()  # breakpoint 035ab9b5 //\n",
    "    Yhat = Z2\n",
    "    \n",
    "    # compute the loss: average cross-entropy loss\n",
    "    loss = cost(Y, Yhat)\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0: \n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "        if loss == lastloss:\n",
    "            break\n",
    "        lastloss = loss\n",
    "    \n",
    "    # backpropagation\n",
    "    E2 = (Yhat - Y )/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1[Z1 <= 0] = 0 # gradient of ReLU \n",
    "    dW1 = np.dot(X, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient Descent update \n",
    "    # import pdb; pdb.set_trace()  # breakpoint 47741f63 //\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2963)\n",
      "[[-3.34721476e+01 -3.91515073e+01 -1.33412545e-01 -2.23907230e+02\n",
      "  -1.97524443e+01 -3.79629727e-02 -2.74093146e+02  6.74069968e-04\n",
      "  -1.33881304e-01 -3.88387225e-02 -2.80647645e+02 -5.17192927e-02\n",
      "   9.23799461e-03 -2.82048328e-02 -5.20493440e-02 -1.99982978e+01\n",
      "  -1.60640453e-03 -4.89221970e+01 -3.80071871e+02 -1.31179429e-02\n",
      "  -2.24972082e-03 -8.69503228e+01 -1.75937465e+02 -2.72463154e+02\n",
      "  -3.92446622e-03  3.09758464e-03  6.34827287e-03 -2.13838094e-03\n",
      "  -1.04179224e-01  8.76911532e-03]\n",
      " [-3.30726457e+01 -3.86973097e+01 -8.55694117e-02 -2.21275518e+02\n",
      "  -1.95425460e+01 -3.02766905e-02 -2.70920992e+02  1.25927509e-02\n",
      "  -1.01378948e-01 -1.71731478e-02 -2.77390594e+02 -9.98093570e-03\n",
      "   8.94302346e-03  5.64082569e-05 -2.74989238e-02 -1.97885513e+01\n",
      "   1.00541971e-02 -4.83598648e+01 -3.75606461e+02  1.78250191e-03\n",
      "  -2.15121595e-02 -8.60176239e+01 -1.73846263e+02 -2.69319355e+02\n",
      "   9.43024131e-03  3.29685221e-03 -1.52555831e-02 -8.71559912e-03\n",
      "  -6.33738485e-02 -2.43798705e-03]\n",
      " [-4.42939383e+02 -5.18474667e+02 -9.98335783e-01 -2.96248397e+03\n",
      "  -2.61529942e+02 -4.16488145e-01 -3.62725593e+03 -2.79461784e-02\n",
      "  -1.10670876e+00 -9.86907677e-02 -3.71415421e+03 -2.02158154e-02\n",
      "  -7.35027150e-03 -2.93108375e-02 -1.74174270e-01 -2.64782169e+02\n",
      "  -1.23582371e-03 -6.47348296e+02 -5.02878533e+03 -3.94408110e-03\n",
      "  -1.13829804e-02 -1.15198141e+03 -2.32770964e+03 -3.60584473e+03\n",
      "  -8.26250077e-03 -9.42327964e-03 -2.22433792e-02 -1.67187947e-04\n",
      "  -8.53240457e-01 -1.26873567e-03]\n",
      " [-9.30478699e+01 -1.08868678e+02 -3.57646315e-01 -6.22530688e+02\n",
      "  -5.48942808e+01 -1.39081097e-01 -7.62147662e+02 -1.03500509e-02\n",
      "  -3.46058822e-01 -8.85256960e-02 -7.80383490e+02 -9.32579432e-02\n",
      "  -1.22983820e-02 -5.98501347e-03 -9.80981508e-02 -5.56351251e+01\n",
      "  -1.16732820e-02 -1.36032545e+02 -1.05670437e+03 -5.60740657e-03\n",
      "  -8.25931739e-03 -2.41860257e+02 -4.89135597e+02 -7.57590569e+02\n",
      "  -1.15713028e-02 -5.31844788e-03 -1.26288944e-02 -1.32173249e-03\n",
      "  -3.12679782e-01 -1.10268819e-02]\n",
      " [-7.53170237e-01 -8.69201555e-01 -1.54660737e-02 -4.95504018e+00\n",
      "  -4.60427555e-01  1.10616479e-02 -6.06255154e+00  1.02022894e-02\n",
      "   6.02239017e-03 -7.75438187e-03 -6.17954046e+00 -1.27849417e-02\n",
      "  -9.45001932e-03  5.07684525e-03 -1.20229585e-02 -4.46587932e-01\n",
      "  -6.66557401e-03 -1.07133160e+00 -8.39158043e+00  7.37656306e-03\n",
      "   1.05341105e-02 -1.92902395e+00 -3.87233194e+00 -6.01427303e+00\n",
      "  -6.10148881e-03  6.70654198e-03  1.63055245e-03  2.27764604e-02\n",
      "   1.53361550e-02 -5.84227242e-03]\n",
      " [-1.84237022e+00 -2.16905250e+00 -1.13905557e-02 -1.24055208e+01\n",
      "  -1.08140857e+00 -1.64413121e-02 -1.51798195e+01  4.59607037e-03\n",
      "  -1.48196620e-02 -3.77438044e-03 -1.55661788e+01  1.02316465e-03\n",
      "   5.87324541e-03 -1.25074199e-02 -8.08588000e-03 -1.11318427e+00\n",
      "  -1.54377890e-02 -2.72026300e+00 -2.10353999e+01  1.53601653e-03\n",
      "  -1.41228630e-02 -4.82034396e+00 -9.73359836e+00 -1.50888643e+01\n",
      "   6.45597653e-03  1.04961139e-03  7.70869053e-03  1.14108693e-02\n",
      "  -6.41019197e-04  6.47189247e-03]\n",
      " [-1.86631327e-01 -1.96163793e-01 -4.84246142e-03 -1.11350764e+00\n",
      "  -1.00469879e-01  3.07203603e-03 -1.38124559e+00  6.42997585e-03\n",
      "   6.15778291e-04 -8.52539211e-03 -1.38457581e+00 -1.60638305e-02\n",
      "  -7.97633534e-03 -1.03996397e-02 -1.64077648e-03 -1.10578091e-01\n",
      "   1.15303962e-02 -2.66291955e-01 -1.90741872e+00 -9.42752578e-04\n",
      "   8.01269603e-03 -4.25706567e-01 -8.61860759e-01 -1.33970067e+00\n",
      "  -1.41020594e-02  6.51726601e-03  2.57470915e-02  1.37635539e-03\n",
      "  -6.57451256e-04  2.79392124e-03]]\n",
      "[[-2.77804940e+00]\n",
      " [-3.25008556e+00]\n",
      " [-1.06061554e-02]\n",
      " [-1.85835724e+01]\n",
      " [-1.63924417e+00]\n",
      " [-4.44957198e-03]\n",
      " [-2.27518478e+01]\n",
      " [-2.22747653e-04]\n",
      " [-1.02497088e-02]\n",
      " [-2.80610769e-03]\n",
      " [-2.32960620e+01]\n",
      " [-2.40240808e-03]\n",
      " [-2.39503848e-04]\n",
      " [-5.23419938e-04]\n",
      " [-2.72186977e-03]\n",
      " [-1.66084004e+00]\n",
      " [ 0.00000000e+00]\n",
      " [-4.06076955e+00]\n",
      " [-3.15447637e+01]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [-7.22025432e+00]\n",
      " [-1.46016718e+01]\n",
      " [-2.26157298e+01]\n",
      " [ 0.00000000e+00]\n",
      " [ 0.00000000e+00]\n",
      " [-2.68183589e-04]\n",
      " [ 0.00000000e+00]\n",
      " [-9.07622138e-03]\n",
      " [-1.22560152e-05]]\n",
      "[[-2.08113821e+02]\n",
      " [-2.90765525e+03]\n",
      " [ 4.11460908e-01]\n",
      " [-1.32799766e+03]\n",
      " [-3.51158968e+02]\n",
      " [ 7.19149991e-01]\n",
      " [-3.46843415e+03]\n",
      " [ 9.58782672e-03]\n",
      " [ 1.85651365e-01]\n",
      " [ 5.10614878e-02]\n",
      " [-4.64684637e+03]\n",
      " [ 5.93117240e-01]\n",
      " [ 5.32684017e-04]\n",
      " [ 3.63860722e-02]\n",
      " [ 8.31278461e-03]\n",
      " [-1.64518891e+02]\n",
      " [ 6.24042956e-03]\n",
      " [-2.99103024e+02]\n",
      " [-2.51980416e+03]\n",
      " [ 1.67247479e-03]\n",
      " [-9.92495137e-03]\n",
      " [-7.56710181e+03]\n",
      " [-8.03980265e+02]\n",
      " [-5.80132656e+03]\n",
      " [-7.48555223e-03]\n",
      " [ 1.68710737e-04]\n",
      " [ 1.31334084e-01]\n",
      " [ 3.60743982e-03]\n",
      " [ 4.22059933e-01]\n",
      " [ 1.64391331e-02]]\n",
      "[[6.26384033]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(W1)\n",
    "print(b1)\n",
    "print(W2)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.26384033 6.26384033 6.26384033 ... 6.26384033 6.26384033 6.26384033]]\n",
      "577.1072967645975\n",
      "Train R-square: -4.440892098500626e-16\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X) + b1 \n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "# import pdb; pdb.set_trace()  # breakpoint 035ab9b5 //\n",
    "Yhat = Z2\n",
    "print(Yhat)\n",
    "print(np.linalg.norm(Y - Y.mean()) ** 2)\n",
    "train_score = 1 - np.linalg.norm(Y - Yhat) ** 2 / np.linalg.norm(Y - Y.mean()) ** 2\n",
    "print(\"Train R-square: {0}\".format(train_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "297/297 [==============================] - 1s 1ms/step - loss: 1.0842 - accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.4111 - accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.4785 - accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.3005 - accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.6083 - accuracy: 0.0000e+00\n",
      "Epoch 8/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.3965 - accuracy: 0.0000e+00\n",
      "Epoch 9/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2200 - accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.4464 - accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.3021 - accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.0000e+00\n",
      "Epoch 19/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2704 - accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.3106 - accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.3266 - accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2361 - accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.2723 - accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2148 - accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2378 - accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.0000e+00\n",
      "Epoch 31/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.4675 - accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1895 - accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.0000e+00\n",
      "Epoch 37/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2342 - accuracy: 0.0000e+00\n",
      "Epoch 38/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1866 - accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2911 - accuracy: 0.0000e+00\n",
      "Epoch 45/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2589 - accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.0000e+00\n",
      "Epoch 53/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.0000e+00\n",
      "Epoch 54/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.0000e+00\n",
      "Epoch 55/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.0000e+00\n",
      "Epoch 59/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2539 - accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2110 - accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.0000e+00\n",
      "Epoch 78/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1773 - accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1842 - accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "297/297 [==============================] - 1s 2ms/step - loss: 0.1737 - accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.2134 - accuracy: 0.0000e+00\n",
      "Epoch 84/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1954 - accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.0000e+00\n",
      "Epoch 88/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1816 - accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1785 - accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.0000e+00\n",
      "Epoch 101/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.0000e+00\n",
      "Epoch 108/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.0000e+00\n",
      "Epoch 109/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1640 - accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.0000e+00\n",
      "Epoch 123/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.0000e+00\n",
      "Epoch 136/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.0000e+00\n",
      "Epoch 137/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.0000e+00\n",
      "Epoch 138/150\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.1695 - accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1562 - accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.0000e+00\n",
      "Epoch 147/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.0000e+00\n",
      "Epoch 149/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x282b624ddb0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load the dataset\n",
    "#dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "#X = dataset[:,0:8]\n",
    "#y = dataset[:,8]\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_shape=(7,), activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X.T, Y.T, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 1ms/step\n",
      "6.2689447\n",
      "0.13719257715336955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_ha = model.predict(X.T)\n",
    "print(Y_ha.mean())\n",
    "print(np.linalg.norm(Y_ha - Y.T)**2 / Y.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6fb85f812c89b156a57296305591057e280590319a8656fd8d970154557d3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
